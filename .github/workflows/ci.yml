name: MLC-LLM CI/CD

on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: write

defaults:
  run:
    shell: bash
    working-directory: .

env:
  IMAGE_NAME: ghcr.io/${{ github.repository }}:latest

jobs:

  docker-build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-qemu-action@v3
      - uses: docker/setup-buildx-action@v3
      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
      - uses: docker/build-push-action@v5
        with:
          context: .
          file: docker/Dockerfile
          push: true
          tags: ${{ env.IMAGE_NAME }}
          platforms: linux/amd64

  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: docker-build
    steps:
      - uses: actions/checkout@v4
      - run: echo "${{ secrets.GHCR_PAT }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
      - run: |
          docker run --rm \
            -e CI=true \
            -v ${{ github.workspace }}:/workspace \
            -w /workspace \
            ${{ env.IMAGE_NAME }} \
            ./scripts/test-image.sh

  build-wheels:
    name: Build Python Wheels
    runs-on: ${{ matrix.os }}
    needs: test
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
      - run: pip install build
      - run: python -m build python/
      - run: |
          WHL=$(ls python/dist/*.whl)
          mv "$WHL" "python/dist/mlc_llm-${{ matrix.os }}.whl"
      - uses: actions/upload-artifact@v4
        with:
          name: wheel-${{ matrix.os }}
          path: python/dist/mlc_llm-${{ matrix.os }}.whl

  release:
    name: Auto Release Every Commit
    runs-on: ubuntu-latest
    needs: build-wheels
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Generate Release Tag
        run: |
          TAG="auto-release-$(date +'%Y%m%d%H%M%S')"
          echo "RELEASE_TAG=$TAG" >> $GITHUB_ENV
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git tag "$TAG"
          git push origin "$TAG"

      - uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ env.RELEASE_TAG }}
          name: Release ${{ env.RELEASE_TAG }}
          files: |
            ./artifacts/wheel-ubuntu-latest/mlc_llm-ubuntu-latest.whl
            ./artifacts/wheel-windows-latest/mlc_llm-windows-latest.whl
            ./artifacts/wheel-macos-latest/mlc_llm-macos-latest.whl
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  deploy-demo-model:
    name: Deploy & Serve Demo Model
    runs-on: ubuntu-latest
    needs: release
    steps:
      - run: echo "${{ secrets.GHCR_PAT }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Start Model Server (CPU)
        run: |
          docker run -d --name mlc-server -p 8000:8000 \
            ${{ env.IMAGE_NAME }} \
            bash -c "
              set -x && \
              mlc_llm download-model --model-name Llama-2-7b-chat-glm-4b-q0f16_0 && \
              mlc_llm serve --model Llama-2-7b-chat-glm-4b-q0f16_0 --device cpu --host 0.0.0.0
            "

      - name: Wait for Model Server
        run: |
          for i in {1..30}; do
            echo "Waiting for server on attempt $i..."
            curl -fsS http://localhost:8000/ && break || sleep 5
          done

      - name: Debug Container Logs (if failed)
        if: failure()
        run: docker logs mlc-server || true

      - name: Validate Chat Completion
        continue-on-error: true
        run: |
          curl -X POST http://localhost:8000/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{
              "model": "Llama-2-7b-chat-glm-4b-q0f16_0",
              "messages": [{"role": "user", "content": "Hello, who are you?"}]
            }' | jq .

      - name: Cleanup Container
        if: always()
        run: |
          echo "Dumping logs before cleanup..."
          docker logs mlc-server || true
          docker rm -f mlc-server || true
